{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fakharyar Khan and Colin Hwang\n",
    "#ECE-475: Frequentist Machine Learning\n",
    "#Project 3: Model Assessment and Selection\n",
    "#Professor Keene\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a randomly generated feature set and classification labels\n",
    "#and sorts the features by their correlation with the classification labels\n",
    "\n",
    "def sim_gen_dat():\n",
    "  #generate a matrix whose entries are samples from a Gaussian distribution of\n",
    "  #mean 0 and std dev 1\n",
    "  #the first 5000 columns are the \"features\" of our dataset\n",
    "\n",
    "  dataset = np.random.normal(0, 1, size = (50, 5000))\n",
    "\n",
    "  #and the last will hold our class labels\n",
    "  labels = np.array(random.choices([0, 1], k = 50))\n",
    "\n",
    "  #get a list of the correlations between the columns of dataset and the class labels\n",
    "\n",
    "  corr_val = [abs(np.correlate(dataset[:, col], labels)[0])*-1 for col in range(0, 5000)]\n",
    "\n",
    "  #use argsort to get the indices of the sorted correlation array\n",
    "  #needed to sort it in descending order so we multiplied the array by -1\n",
    "  indices = np.argsort(corr_val)\n",
    "\n",
    "\n",
    "  #then we rearange the columns dataset based on these indices\n",
    "  #in other words, we sort the features in the matrix based on their correlation they\n",
    "  #are to the class label\n",
    "\n",
    "  dataset[:] = dataset[:, indices]\n",
    "\n",
    "  return dataset, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the top 100 features that are most correlated with the label\n",
    "#but doesn't use the validation set since that would give the model\n",
    "#information about the testing labels\n",
    "\n",
    "#takes in the feature matrix, the class labels, and the indices of the feature\n",
    "#and label matrix corresponding to the training set\n",
    "def best_feat(feat_mat, labels, train):\n",
    "\n",
    "    #extract the training set from the feature matrix and labels\n",
    "    train_x = feat_mat[train]\n",
    "    train_y = labels[train]\n",
    "\n",
    "    #then just as before, we take the top 100 features in the matrix\n",
    "    #that are most correlated with the labels\n",
    "\n",
    "    corr_val = [abs(np.correlate(train_x[:, col], labels)[0])*-1 for col in range(0, 5000)]\n",
    "\n",
    "\n",
    "    indices = np.argsort(corr_val)\n",
    "\n",
    "    feat_mat[:] = feat_mat[:, indices]\n",
    "\n",
    "    feat_mat = feat_mat[:, 0:100]\n",
    "    return feat_mat\n",
    "    \n",
    "\n",
    "#this function performs 5-fold cross validation\n",
    "#and returns the average score that the model (KNN)\n",
    "#achieves which gives us an idea of how well the \n",
    "#model will perform on the testing set\n",
    "\n",
    "#takes in a boolean bad which tells it whether or not\n",
    "#to perform the cross validation incorrectly (True) or correctly (False)\n",
    "\n",
    "def run_sim(bad):\n",
    "    #get the features and labels\n",
    "    feat_mat, labels = sim_gen_dat()\n",
    "\n",
    "    #if we're doing cross validation wrong\n",
    "    if(bad):\n",
    "        #just take the top 100 most correlated features\n",
    "        #which were determined using the entire dataset\n",
    "        feat_mat = feat_mat[:, 0:100]\n",
    "\n",
    "    avg_score = 0\n",
    "\n",
    "    #create an instance of the KNN classifier\n",
    "    neigh_mod = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "    #split the data into 5 folds\n",
    "    kf = KFold(n_splits = 5)\n",
    "    \n",
    "    #then we iterate through each split by having\n",
    "    #each fold be the validation set and the rest be the training set\n",
    "\n",
    "    for train, val in kf.split(range(50)):\n",
    "        \n",
    "        feat = feat_mat\n",
    "\n",
    "        #if we're doing it the right way\n",
    "        if(not bad):\n",
    "            #we determine the top 100 features using the 4 folds of training data\n",
    "            #that we have by calling our best_feat function\n",
    "            feat = best_feat(feat_mat, labels, train)\n",
    "\n",
    "        #get the features in the validation and training sets\n",
    "        #and the labels as well using the indices given in train and val\n",
    "\n",
    "        val_x = feat[val]\n",
    "        val_y = labels[val]\n",
    "\n",
    "        train_x = feat[train]\n",
    "        train_y = labels[train]\n",
    "\n",
    "        #fit our model to the training data\n",
    "        neigh_mod.fit(train_x, train_y)\n",
    "\n",
    "        #get the score and add it\n",
    "        avg_score += neigh_mod.score(val_x, val_y)\n",
    "    \n",
    "    #get the average performance of the model\n",
    "    return avg_score / 5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, the bad method of cross validation believes that we will achieve an error\n",
      "rate of  0.13360000000000016  when we train and test our model on a randomly generated dataset\n",
      "\n",
      "\n",
      "\n",
      "On the other hand, the good method of cross validation tells us that on average, we will achieve an error\n",
      "rate of  0.51  when we train and test our model on a randomly generated dataset\n"
     ]
    }
   ],
   "source": [
    "#here we will test out the two different methods of validation\n",
    "#and see what they tell us what kind of error we should expect\n",
    "#when we train and test a KNN model on a randomly generated dataset\n",
    "\n",
    "avg_score_bad = 0\n",
    "\n",
    "avg_score_good = 0\n",
    "\n",
    "for iterations in range(50):\n",
    "    avg_score_bad += run_sim(True)\n",
    "    avg_score_good += run_sim(False)\n",
    "\n",
    "print(\"On average, the bad method of cross validation believes that we will achieve an error\")\n",
    "print(\"rate of \" , 1 - avg_score_bad/50, \" when we train and test our model on a randomly generated dataset\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"On the other hand, the good method of cross validation tells us that on average, we will achieve an error\")\n",
    "print(\"rate of \" , 1 - avg_score_good/50, \" when we train and test our model on a randomly generated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY\n",
    "\n",
    "In this project, we learned the right and the wrong way to do cross-validation. If given N = 50 samples with two equally sized classes and p = 5000 features that are independent of the class labels, we should expect to obtain an error rate of around 50% for the classifier. However, when you screen the best predictions before building the classifier and using cross validation to estimate tuning parameters, the prediction error will be much lower than expected.\n",
    "\n",
    "Using the method outlined by the textbook (choosing the 100 predictors with the highest correlation, then using a 1-nearest neighbor classifier based on the predictors), the textbook obtained an error rate of 3% and in our implementation, we obtained an error rate of approximately 13%, which indicates this method is flawed. This is because when the predictors were chosen, they were chosen on the basis of all samples, leaving samples out only after the predictors had been chosen. This does not mirror how a classifier should operate on an independent test set since the predictors were allowed to \"see\" samples that were left out. \n",
    "\n",
    "Therefore, in order to obtain an error rate close to the theoretical, ideal error rate of 50%, we must choose the best predictions based on only the training data - not the testing - when performing cross validation. First, we divide the samples into K cross-validation folds randomly, where we chose 5 folds to follow the textbooks implementation. Then, for each of the 5 folds, we found a subset of good predictors and used the subset of predictors to construct a multivariate classifer, using all samples except samples within fold K. We then used the classifier to predict labels for the samples in fold K. The error estimates that are calculated from predicting the labels in all of the folds are accumulated in order to obtain the cross-validation of the error rate. This method prevents selected predictors from \"seeing\" leftout samples. Using this method, we got an error rate of ~50%, which is the exact value we should expect for this problem. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "727a46c3a7188efcfd431d7033575d745715cf5c05f73771b8eba79015da05ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
