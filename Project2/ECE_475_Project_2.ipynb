{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHEj6TWk0E2a"
      },
      "outputs": [],
      "source": [
        "#Colin Hwang and Fakharyar Khan\n",
        "#ECE-475: Frequentist Machine Learning\n",
        "#Project 2: Logistic Regression\n",
        "#Professor Keene\n",
        "\n",
        "#importing all the necessary datatypes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import seaborn as sns\n",
        "import operator\n",
        "from matplotlib import pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracts the dataset from the csvfile\n",
        "#splits dataset into training, validation, and testing sets\n",
        "#takes in the filename of the csvfile, the name of the label for the dataset, and a list of the names of all of the cateogrical features\n",
        "\n",
        "def prepare_data(filename, label, categ):\n",
        "\n",
        "  #our processing function didn't generalize completely to all of the datasets\n",
        "  #so we needed to process each dataset a little bit differently\n",
        "\n",
        "  #if on the SA heart disease dataset\n",
        "  if(label == \"chd\"):\n",
        "\n",
        "    #make the first column the index\n",
        "    df = pd.read_csv(filename, index_col = [0])\n",
        "\n",
        "    #and because it contains a non-numerical feature, family history, we can use\n",
        "    #one-hot encoding to make it numerical\n",
        "\n",
        "    df.famhist = df.famhist.replace({\"Present\": 1, \"Absent\": 0})\n",
        "  \n",
        "  #if on the beans dataset\n",
        "  elif label == \"Class\":\n",
        "\n",
        "    #the csv file uses an encoding different from the one read_csv assumes by default\n",
        "    df = pd.read_csv(filename, encoding= 'unicode_escape')\n",
        "\n",
        "    #uses one hot encoding to encode the different classes of the label as binary strings\n",
        "    df.Class = df.Class.replace({\"SEKER\": \"000\", \"BARBUNYA\": \"001\", \"BOMBAY\":\"010\",\n",
        "                                 \"CALI\": \"011\", \"DERMASON\": \"100\", \"HOROZ\":\"101\", \"SIRA\":\"110\"})\n",
        "    \n",
        "  \n",
        "  elif label == \"stabf\":\n",
        "    df = pd.read_csv(filename)\n",
        "    df.stabf = df.stabf.replace({\"unstable\": 1, \"stable\": 0})\n",
        "  \n",
        "  #else we're reading from the heart failure dataset for which we can just use the default parameters of the \n",
        "  #read_csv function\n",
        "\n",
        "  else:\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  #split the dataset into training (80%), testing (10%), and validation (10%) sets\n",
        "  train, test = train_test_split(df, test_size = 0.2)\n",
        "\n",
        "  validation, test = train_test_split(test, test_size = 0.5)\n",
        "\n",
        "  \n",
        "  #extract the labels from each set which will create two dataframes for each set\n",
        "  #Our notation for these dataframes is: \n",
        "  # {type_data}_{type_set} where type_data indicates whether the dataframe contains features (x)\n",
        "  #or labels (y) and type_set indicates what set the dataframe belongs to (train, test, validation)\n",
        "\n",
        "  ytrain = train[label]\n",
        "  xtrain = train.drop(label, axis = 1)\n",
        "  \n",
        "  ytest = test[label]\n",
        "  xtest = test.drop(label, axis = 1)\n",
        "\n",
        "  vtest = validation[label]\n",
        "  vtrain = validation.drop(label, axis = 1)\n",
        "\n",
        "  #before normalizing the feature dataframes, we take out the categorical features\n",
        "\n",
        "  xtrain_categ = train[categ]\n",
        "  xtest_categ = test[categ]\n",
        "  vtrain_categ = validation[categ]\n",
        "\n",
        "  xtrain = xtrain.drop(categ, axis = 1)\n",
        "  xtest = xtest.drop(categ, axis = 1)\n",
        "  vtrain = vtrain.drop(categ, axis = 1)\n",
        "\n",
        "\n",
        "  #normalizing the numerical features\n",
        "  xtrain = (xtrain - xtrain.mean())/xtrain.std()\n",
        "  xtest = (xtest - xtest.mean())/xtest.std()\n",
        "  vtrain = (vtrain - vtrain.mean())/vtrain.std()\n",
        "\n",
        "  \n",
        "  #and after we're done normalizing them, we put the categorical features back in\n",
        "  xtrain = pd.concat([xtrain, xtrain_categ], axis = 1)\n",
        "  xtest = pd.concat([xtest, xtest_categ], axis =1)\n",
        "  vtrain = pd.concat([vtrain, vtrain_categ], axis =1)\n",
        "\n",
        "  \n",
        "\n",
        "  return xtrain, ytrain, xtest, ytest, vtrain, vtest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0xPh4UDP0_Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This code block is basically where the classification algorithms (except for stretch goal #2) are implemented\n",
        "\n",
        "#this is our update function\n",
        "#it updates all of the weights using stochastic gradient descent \n",
        "\n",
        "#it takes in: the old weight vector, beta_old, the features of a single datapoint, xi,\n",
        "#and its label, yi, the lambda paramter for regularization, lamb, the step size for\n",
        "#gradient descent, alpha, and L2, a boolean that tells us whether to do L2 or L1 regularization\n",
        "\n",
        "#it returns the updated weight vector\n",
        "def step(beta_old, xi, yi, lamb, alpha, L2):\n",
        "\n",
        "  #first update the weight vector by taking a step in the direction of the gradient\n",
        "  #of the log_likelihood function\n",
        "\n",
        "  beta_new = beta_old + alpha*(yi - hyp_func(beta_old, xi))*xi\n",
        "\n",
        "  #then if we're doing L2 regularization\n",
        "  if L2:\n",
        "    #we add in the gradient of the L2 penalty\n",
        "    return  beta_new - [beta*lamb for beta in beta_old]\n",
        "  \n",
        "  #else we're doing L1 regularization\n",
        "  #since the L1 penalty isn't differentiable at 0 we can't just take the gradient\n",
        "  #of the penalty to get the update vector\n",
        "  #instead we used one of the weight update procedures described in this paper: https://aclanthology.org/P09-1054.pdf\n",
        "  #called SGD-L1 (Clipping)\n",
        "  \n",
        "  #in this method after adding in the update component from the likelihood function\n",
        "  #you add the derivative (+- lambda) of the L1 penalty but if there's a sign change, the weight gets clipped to 0\n",
        "  \n",
        "  return [max(0, beta - alpha*lamb) if beta > 0 else min(0, beta + alpha*lamb) for beta in beta_new]\n",
        "\n",
        "\n",
        "#hyp_func evaluates the sigma function at the dot product of beta and x, the list of feature values\n",
        "#for a datapoint\n",
        "#its output is the conditional probability that the datapoint is a 1 given its feature values\n",
        "\n",
        "def hyp_func(beta, x):\n",
        "\n",
        "  #take the transpose of beta\n",
        "  beta_tran = np.transpose(beta)\n",
        "\n",
        "  #return the sigmoid function evaluated at the matrix product of the transpose of beta\n",
        "  #and x (or the dot product)\n",
        "\n",
        "  return (1 + math.exp(-1*np.matmul(beta_tran, x)))**-1\n",
        "\n",
        "#here we fit the model\n",
        "#by going through every data point in the training set\n",
        "#and updating the weight vectors using the step function\n",
        "\n",
        "#takes in x, the training set, y the list of labels,\n",
        "#alpha, the step size parameter, and L2 a boolean that tells use\n",
        "#if we're performing L2 or L1 regularization\n",
        "\n",
        "def fit_model(x, y, lamb, alpha, L2):\n",
        "\n",
        "  #make space for a vector that has as many entries as there are features\n",
        "  #it's advised to start SGD with all of the weights as 0\n",
        "\n",
        "  beta = [0]*np.shape(x)[1]\n",
        "  \n",
        "  #go through every data point\n",
        "  #and update the weights using the step function\n",
        "\n",
        "  for index in range(np.shape(x)[0]):\n",
        "    beta = step(beta, x[index], y[index], lamb, alpha, L2)\n",
        "\n",
        "  return beta\n",
        "\n",
        "# calculates the log likelihood of our dataset occuring in the probability distribution\n",
        "# predicted by our model if we maximize this value, assuming our training set isn't biased,\n",
        "# we would have found an estimate for the probability distribution of the sample space\n",
        "\n",
        "#takes in the the weight vector calculated by the model, beta, the feature values, x,\n",
        "#and the labels, y\n",
        "\n",
        "#this function doesn't actually get used in the code. We were going to use it\n",
        "#to access the performance of the model on the validation set for when we needed to\n",
        "#tune the regularization parameters and perform cross validation\n",
        "#but we found that using the percent of correct classification on the training set \n",
        "#yielded better results for all of the datasets we used\n",
        "\n",
        "def log_likelihood(beta, x, y):\n",
        "\n",
        "  prob = 0\n",
        "\n",
        "  #go through every data point\n",
        "  for index in range(np.shape(x)[0]):\n",
        "\n",
        "    #and since each datapoint is independent, we can multiply the probability of them occuring\n",
        "    #together but since we took the log of the likelihood, we add them instead\n",
        "\n",
        "    prob += y[index]*math.log(hyp_func(beta, x[index]))\n",
        "    \n",
        "    prob += (1 - y[index])*math.log(1- hyp_func(beta, x[index]))\n",
        "\n",
        "  \n",
        "  return math.exp(prob)\n",
        "\n",
        "#the predict function sees how well our model does at classifying\n",
        "#the testing/validation set\n",
        "#calculates the percent of the set that the model correctly predicted\n",
        "\n",
        "#takes in weight vector calculated by model, beta, feature values, and labels\n",
        "\n",
        "def predict(beta, x, y):\n",
        "\n",
        "  numCorrect  = 0\n",
        "\n",
        "  #go through each datapoint\n",
        "  for index in range(np.shape(x)[0]):\n",
        "\n",
        "    prediction = 0\n",
        "    #since hyp_func calculates probability of y being 1 given x,\n",
        "    #if it's > 0.5, we can say that it's more likely than not that the label is 1\n",
        "    #if it's < 0.5, then we say that the model predicted a label of 0\n",
        "\n",
        "    proby1 = hyp_func(beta, x[index])\n",
        "    if proby1 > 0.5:\n",
        "      prediction = 1\n",
        "    \n",
        "    #if we got it right, add one to numCorrect\n",
        "    if prediction == y[index]:\n",
        "      numCorrect += 1\n",
        "  \n",
        "  #find percent correct\n",
        "  return 100* numCorrect/len(y)\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "HNlF1MPf6jqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this driver function pulls all of the functions in the previous code block\n",
        "#to generate and test the performance of the model\n",
        "\n",
        "#takes in a training set with features in trainx, and labels in trainy\n",
        "#a testing/validation set with features in testx and labels in testy\n",
        "#lamb, the regularization parameter, L2, a boolean that tells it to do L2 or L1 regularization\n",
        "#and output, a boolean that tells the function to output the percent correct if false\n",
        "#or return the weights and percent correct if true\n",
        "\n",
        "def driver(trainx, trainy, testx, testy, lamb, alpha, L2, output):\n",
        "\n",
        "  #convert trainx and trainy into numpy arrays\n",
        "  xtrain_mat = trainx.to_numpy()\n",
        "\n",
        "  size = np.size(xtrain_mat, 0)\n",
        "\n",
        "  #and append a column of ones to xtrain_mat so that we have a weight\n",
        "  #that represents the y-intercept\n",
        "\n",
        "  xtrain_mat = np.c_[np.ones(size), xtrain_mat]\n",
        "\n",
        "  ytrain_mat = trainy.to_numpy()\n",
        "  \n",
        "  #train the model on the training set and get the weight vector\n",
        "  beta = fit_model(xtrain_mat, ytrain_mat, lamb, alpha, L2)\n",
        "\n",
        "  #now we need to see how it does on the testing set\n",
        "  #convert xtest and ytest into numpy arrays\n",
        "  #and append a column of 1s to xtest_mat\n",
        "\n",
        "  xtest_mat = testx.to_numpy()\n",
        "\n",
        "  size = np.size(xtest_mat, 0)\n",
        "\n",
        "  xtest_mat = np.c_[np.ones(size), xtest_mat]\n",
        "  ytest_mat = testy.to_numpy()\n",
        "\n",
        "  #call on predict function to get the percent of labels in testing set\n",
        "  #that model predicted correctly\n",
        "\n",
        "  score = predict(beta, xtest_mat, ytest_mat)\n",
        "  likelihood = log_likelihood(beta, xtest_mat, ytest_mat)\n",
        "\n",
        "  #if output is true, return the weights and the score\n",
        "  if(output):\n",
        "    return beta, score\n",
        "\n",
        "  #else just return the score\n",
        "  return score\n",
        "\n",
        "#this function impements the forward stepwise feature selection algorithm\n",
        "#it selects the most important group of features in the following way:\n",
        "\n",
        "#build all of the one-feature models. Choose the best k of them\n",
        "#build all 2-feature models from the k best one-feature models\n",
        "#build all 3-feature models from the k best two feature models and so on\n",
        "\n",
        "#takes in the regularization parameter, lamb, the step parameter, alpha,\n",
        "#the number of models to choose in each iteration, num_mod, and the number of \n",
        "#iterations of the algorithm the function should perform, num_feat\n",
        "\n",
        "#outputs the num_feat most important features according to the function\n",
        "\n",
        "def forward_selection(alpha, num_mod, num_feat):\n",
        "\n",
        "  #create two dictionaries\n",
        "  #they will both map a score to the model (a list of features used in the model, betas can be reproduced)\n",
        "  #it's possible that in an iteration, two models can end up having the same score\n",
        "  #and one would overwrite the other in the dictionary but that's really unlikely\n",
        "\n",
        "  feat_dict = {}\n",
        "  feat_dict_new = {}\n",
        "\n",
        "  #assign the null model a score of 0 (or ig the other way around)\n",
        "  #since u can only get a value of 0 from the null model, for every datapoint\n",
        "  #it always gives a probability of 0.5 for the label being a 1 which isn't a decision\n",
        "  #in any direction so it couldn't technically predict any of the labels correctly\n",
        "\n",
        "  feat_dict[0] = []\n",
        "\n",
        "  #get the set of the names of all of the features\n",
        "  all_features = set(xtrain.columns)\n",
        "\n",
        "\n",
        "  #in each iteration\n",
        "  for iter in range(num_feat):\n",
        "\n",
        "    #feat_dict_new will hold the performance of the models from the current iteration\n",
        "    #feat_dict holds the values from the previous iteraton\n",
        "\n",
        "\n",
        "\n",
        "    #empty out feat_dict_new\n",
        "    feat_dict_new = {}\n",
        "\n",
        "    #iterate through the models from the previous iteration (the keys are their scores)\n",
        "    for model in feat_dict.keys():\n",
        "\n",
        "      #iterate through every feature that wasn't used by the current model\n",
        "      for feature in all_features - set(feat_dict[model]):\n",
        "\n",
        "        #append that feature to the list of features used by the model\n",
        "        model_chall = feat_dict[model] + [feature]\n",
        "\n",
        "        #use those features to train the model and access its performance on the \n",
        "        #validation set\n",
        "        \n",
        "        score = driver(xtrain[model_chall], ytrain, xval[model_chall], \n",
        "                            \n",
        "                            yval, 0 , alpha, False, False)\n",
        "        \n",
        "        #add the model and its score to feat_dict_new since it's a model from this\n",
        "        #iteration\n",
        "        feat_dict_new[score] = model_chall\n",
        "    \n",
        "    #get a list of the scores of the num_mod best models\n",
        "    best_mod = (sorted(feat_dict_new.keys(), reverse = True))[0: num_mod - 1]\n",
        "\n",
        "    #empty out feat_dict\n",
        "    feat_dict = {}\n",
        "\n",
        "    #and fill it up with the num_mod best models of this iteration\n",
        "    #since in the next iteration they will be the models of the previous iteration\n",
        "    for mod in best_mod:\n",
        "      feat_dict[mod] = feat_dict_new[mod]\n",
        "  \n",
        "  #return the best performing model from the last iteration\n",
        "  return feat_dict[(sorted(feat_dict.keys(), reverse = True))[0]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#sweeps through lambda and finds the best regularization parameter\n",
        "\n",
        "#takes in L2 which determines whether to use L1 or L2 regularization\n",
        "#and plot which tells it whether or not to return coeff, an output necessary \n",
        "#for plotting the weights of the coefficients as a function of lambda\n",
        "\n",
        "def opt_lamb(L2, plot):\n",
        "\n",
        "\n",
        "  opt_lamb = 0\n",
        "\n",
        "  #coeff is a list of lists of weights\n",
        "  #we'll append the weight vectors of each model to here\n",
        "\n",
        "  coeff = []\n",
        "\n",
        "  #create a range of values for lambda to take on\n",
        "  lambdas = np.arange(0, 2, 0.01)\n",
        "\n",
        "  \n",
        "  max_score = 0\n",
        "\n",
        "\n",
        "  beta = []\n",
        "\n",
        "  #sweep through lambda\n",
        "  for lamb in lambdas:\n",
        "\n",
        "    #get the weights and the performance of the model for that lamdba\n",
        "    beta, score = driver(xtrain, ytrain, xval, yval, lamb, 0.0001, L2, True)\n",
        "\n",
        "    #append the weight vector to coeff\n",
        "    coeff.append(beta)\n",
        "    \n",
        "    #if we got a higher score than max, se that to be opt_lamb\n",
        "    if score > max_score:\n",
        "\n",
        "      max_score = score\n",
        "      opt_lamb = lamb\n",
        "\n",
        "  #if plot\n",
        "  if (plot):\n",
        "    \n",
        "    #return best lambda and the weight vector\n",
        "    return opt_lamb, coeff\n",
        "\n",
        "  #else just best lambda\n",
        "  return opt_lamb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "metadata": {
        "id": "A9dqsKKC8sWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#produces the scatterplot matrix of a given dataset\n",
        "#takes in the csv filename and the name of the label\n",
        "\n",
        "def scatterplot(filename, label):  \n",
        "  print(\"SCATTER PLOT MATRIX OF DATASET\\n\\n\")\n",
        "\n",
        "  #if we're working with the SA heart disease dataset\n",
        "  if(label == \"chd\"):\n",
        "    #we have to make the first column the index\n",
        "    df = pd.read_csv(filename, index_col = [0])\n",
        "  else:\n",
        "    #else just read it in normally\n",
        "    df = pd.read_csv(filename)\n",
        "    \n",
        "  #produce the scatterplot\n",
        "  sns.pairplot(df, hue = label)"
      ],
      "metadata": {
        "id": "5yqyQqDAuxE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#main function puts almost everything that we did so far together in one output\n",
        "#it runs four different models that use unregularized logistic regression, regression\n",
        "#optimized for L1 regularization, regression optimized for L2 regularization, and\n",
        "#regression using the forward stepwise feature selection\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "  #calculate the score for unregularized regression\n",
        "  score1 = driver(xtrain, ytrain, xtest, ytest, 0, 0.0001, True, False)\n",
        "\n",
        "  #create a dataframe in which we'll store the scores for each of the models\n",
        "  mod_perf = pd.DataFrame()\n",
        "\n",
        "  #fill in the results for unregularized regression\n",
        "  mod_perf[\"Unregularized\"] = [score1]\n",
        " \n",
        "\n",
        "\n",
        "  #perform forward stepwise feature selection for 4 iterations and \n",
        "  #choosing the 4 best models in each iteration\n",
        "\n",
        "  best_feat = forward_selection(0.0001, 4, 4)\n",
        "\n",
        "  #find out how the model does on that subset of the training set\n",
        "  score2 = driver(xtrain[best_feat], ytrain, xtest[best_feat], ytest, 0, 0.0001, True, False)\n",
        "\n",
        "  #display the most important features chosen by the algorithm\n",
        "  print(\"From the results received from the forward stepwise algorithm, it appears\\nthat the most important features are: \", best_feat)\n",
        "\n",
        "  #log the score into the dataframe\n",
        "\n",
        "  mod_perf[\"Forward Stepwise\"] = [score2]\n",
        "\n",
        "\n",
        "  #get the best lambda for L2 regularization\n",
        "  best_lamb = opt_lamb(True, False)\n",
        "\n",
        "  #get the score of the model\n",
        "  score3 = driver(xtrain, ytrain, xtest, ytest, best_lamb, 0.0001, True, False)\n",
        "\n",
        "  #log it into the df\n",
        "  mod_perf[\"L2 Regularization\"] = [score3]\n",
        "\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  #get best lambda for L1 regularization\n",
        "  best_lamb = opt_lamb(False, False)\n",
        "\n",
        "  \n",
        "\n",
        "  score4 = driver(xtrain, ytrain, xtest, ytest, best_lamb, 0.0001, False, False)\n",
        "\n",
        "  #log it in\n",
        "  mod_perf[\"L1 Regularization\"] = [score4]\n",
        "\n",
        "  #display the dataframe\n",
        "  display(mod_perf)\n"
      ],
      "metadata": {
        "id": "w3olyZ09t0qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function plots the coefficient weights as a function of lambda\n",
        "#the regularization parameter for L1 regularization\n",
        "\n",
        "def coef_plot():\n",
        "\n",
        "  #allocate space for the coefficients of each of the features\n",
        "\n",
        "  sbp = []\n",
        "  tobacco = []\n",
        "  ldl = []\n",
        "  adiposity = []\n",
        "  famhist = []\n",
        "  typea = []\n",
        "  obesity = []\n",
        "  alcohol = []\n",
        "  age = []\n",
        "\n",
        "  #get the best lambda and the coefficient, the list of list of weights\n",
        "\n",
        "  best_lamb, coeff = opt_lamb(False, True)\n",
        "\n",
        "  #go through every list of weights\n",
        "  for weights in coeff:\n",
        "    \n",
        "    #append each weight to the corresponding feature\n",
        "    sbp.append(weights[0])\n",
        "    tobacco.append(weights[1])\n",
        "    ldl.append(weights[2])\n",
        "    adiposity.append(weights[3])\n",
        "    famhist.append(weights[4])\n",
        "    typea.append(weights[5])\n",
        "    obesity.append(weights[6])\n",
        "    alcohol.append(weights[7])\n",
        "    age.append(weights[8])\n",
        "  \n",
        "  #make the range for the values of lambda we swept through\n",
        "  lambdas = np.arange(0, 2, 0.01)\n",
        "\n",
        "  #plot the weights of each feature against lambda\n",
        "  plt.plot(lambdas, sbp, color = 'blue', label = 'sbp')\n",
        "  plt.plot(lambdas, tobacco, color = 'red', label = 'tobacco')\n",
        "  plt.plot(lambdas, adiposity, color = 'green', label = 'adiposity')\n",
        "  plt.plot(lambdas, famhist, color = 'grey', label = 'famhist')\n",
        "  plt.plot(lambdas, typea, color = 'purple', label = 'typea')\n",
        "  plt.plot(lambdas, obesity, color = 'black', label = 'obesity')\n",
        "  plt.plot(lambdas, alcohol, color = 'brown', label = 'alcohol')\n",
        "  plt.plot(lambdas, age, color = 'pink', label = 'age')\n",
        "\n",
        "\n",
        "  plt.title(\"Logisitic Regression Model Coefficients as a Function of Lambda\")\n",
        "\n",
        "  plt.xlabel(\"lambda\")\n",
        "  plt.ylabel(\"Coefficients\")\n",
        "\n",
        "  plt.legend(loc=\"upper right\")\n",
        "\n",
        "  #display results as well as most important features found according to the graph\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  print(\"The optimal lambda found was: \", best_lamb)\n",
        "  print(\"From the lasso plot, it appears that the most important features are\")\n",
        "  print(\"sbp, age, adiposity, and famhist\")\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "FR7Agf0n1EOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we plot the scatterplot matrix for the dataset. This can take around 40 seconds and messes\n",
        "#with the formatting of other plots which is why it gets its own cell block\n",
        "scatterplot(\"SAheart.csv\", \"chd\")"
      ],
      "metadata": {
        "id": "eAOAzhn_W129"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we see the results of all the different experiments we ran on the SAheart dataset\n",
        "#we make the lasso plot, and we call on main to see how the 4 models did\n",
        "\n",
        "\n",
        "xtrain, ytrain, xtest, ytest, xval, yval = prepare_data(\"SAheart.csv\", \"chd\", [\"famhist\"])\n",
        "\n",
        "print(\"--------------RESULTS OF TRAINING MODEL USING L1 REGULARIZATION--------------\")\n",
        "print(\"\\n\\n\")\n",
        "coef_plot()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"--------------COMPARING PERFORMANCE OF MODELS ON DATASET--------------\")\n",
        "\n",
        "print(\"\\n\")\n",
        "main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "kYdc0723y4rG",
        "outputId": "c4496a46-a543-4bc2-a39b-4195bc0daae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------RESULTS OF TRAINING MODEL USING L1 REGULARIZATION--------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "The optimal lambda found was:  0.05\n",
            "From the lasso plot, it appears that the most important features are\n",
            "sbp, age, adiposity, and famhist\n",
            "\n",
            "\n",
            "\n",
            "--------------COMPARING PERFORMANCE OF MODELS ON DATASET--------------\n",
            "\n",
            "\n",
            "From the results received from the forward stepwise algorithm, it appears\n",
            "that the most important features are:  ['adiposity', 'age', 'tobacco', 'ldl']\n",
            "0.0\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unregularized  Forward Stepwise  L2 Regularization  L1 Regularization\n",
              "0      85.106383         89.361702          85.106383          87.234043"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c73c9f35-39de-42ca-ab19-285b83b5e0d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unregularized</th>\n",
              "      <th>Forward Stepwise</th>\n",
              "      <th>L2 Regularization</th>\n",
              "      <th>L1 Regularization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>85.106383</td>\n",
              "      <td>89.361702</td>\n",
              "      <td>85.106383</td>\n",
              "      <td>87.234043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c73c9f35-39de-42ca-ab19-285b83b5e0d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c73c9f35-39de-42ca-ab19-285b83b5e0d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c73c9f35-39de-42ca-ab19-285b83b5e0d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here we try out our algorithm on a different dataset, clinical records showing whether a patient\n",
        "#died of heart failure or not and certain relevant features\n",
        "xtrain, ytrain, xtest, ytest, xval, yval = prepare_data(\"heart_failure_clinical_records_dataset.csv\", \"DEATH_EVENT\", \n",
        "                                                        [\"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\"])\n",
        "\n",
        "print(len(ytest))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "vTJZF6UhDo0Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "cf493f9e-7101-403e-ff7d-6b0dd72b5086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "From the results received from the forward stepwise algorithm, it appears\n",
            "that the most important features are:  ['time', 'ejection_fraction', 'creatinine_phosphokinase', 'platelets']\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unregularized  Forward Stepwise  L2 Regularization  L1 Regularization\n",
              "0      93.333333         86.666667          93.333333               90.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a01c7f5b-0597-4035-a044-57e136723747\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unregularized</th>\n",
              "      <th>Forward Stepwise</th>\n",
              "      <th>L2 Regularization</th>\n",
              "      <th>L1 Regularization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>93.333333</td>\n",
              "      <td>86.666667</td>\n",
              "      <td>93.333333</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a01c7f5b-0597-4035-a044-57e136723747')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a01c7f5b-0597-4035-a044-57e136723747 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a01c7f5b-0597-4035-a044-57e136723747');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here we plot the scatterplot matrix for the heart failure dataset\n",
        "\n",
        "scatterplot(\"heart_failure_clinical_records_dataset.csv\", \"DEATH_EVENT\")"
      ],
      "metadata": {
        "id": "U7ZeWGCbVGab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Qn7w_c6Ngp-z",
        "outputId": "201548b4-09a7-4bbb-c572-c4e56f2fea4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From the results received from the forward stepwise algorithm, it appears\n",
            "that the most important features are:  ['g3', 'p4', 'p2', 'stab']\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unregularized  Forward Stepwise  L2 Regularization  L1 Regularization\n",
              "0           93.4              95.6              0.934              0.934"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f09ffd6-fe62-49e4-98a4-144ff702dda4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unregularized</th>\n",
              "      <th>Forward Stepwise</th>\n",
              "      <th>L2 Regularization</th>\n",
              "      <th>L1 Regularization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>93.4</td>\n",
              "      <td>95.6</td>\n",
              "      <td>0.934</td>\n",
              "      <td>0.934</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f09ffd6-fe62-49e4-98a4-144ff702dda4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f09ffd6-fe62-49e4-98a4-144ff702dda4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f09ffd6-fe62-49e4-98a4-144ff702dda4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here is our implementation of multinomial classification on the dry beans dataset\n",
        "#there wasn't any good way of editing functions like step, hyp_func, and fit_model\n",
        "#so that they worked for binomial and multinomial classification so we just redefined\n",
        "#them for multinomial classification here\n",
        "\n",
        "xtrain, ytrain, xtest, ytest, xval, yval = prepare_data(\"Dry_Beans_Dataset.csv\", \"Class\", [])\n",
        "\n",
        "#The main difference between binomial and multinomial classification is that \n",
        "#now each class gets its own weight vector and the hyp_function is a little bit different\n",
        "#but other than that it's pretty similar\n",
        "\n",
        "#step function updates a certain weight vector \n",
        "#takes in index, which tells it which weight vector to update\n",
        "#xi, the features of a datapoint, yi, the label of that datapoint\n",
        "#alpha, the step size, and W the ist of weight vectors\n",
        "\n",
        "def step(index, xi, yi, alpha, W):\n",
        "\n",
        "  #so if we are calculating update function for the weight vector corresponding\n",
        "  #to the jth class, ti is 0 when i != j and 1 when i == j\n",
        "\n",
        "  ti = 0\n",
        "  #since our labels are binary strings from 0 to 6\n",
        "  #if we convert them to decimal we can check which class we're in since the index\n",
        "  #corresponds to the class too\n",
        "\n",
        "  if int(yi, 2) == index:\n",
        "    ti = 1\n",
        "\n",
        "  #once we find out ti, we updat ethe function just as before\n",
        "  return W[index] + alpha*(ti - hyp_func(W[index], W, xi))*xi\n",
        "\n",
        "#hyp_func is the other thing that changes\n",
        "#it takes in the weight vector, beta,\n",
        "#W, the weight matrix, and x, then feature values of the datapoint\n",
        "\n",
        "def hyp_func(beta, W, x):\n",
        "\n",
        "  #transpose beta\n",
        "  beta_tran = np.transpose(beta)\n",
        "\n",
        "  denom = 0\n",
        "\n",
        "  #the denominator of the hyp_func is the sum of e raised to the power\n",
        "  #of the dot product of each of the weight vectors with x\n",
        "\n",
        "  #iterate through each of the weight vectors\n",
        "  for weights in W:\n",
        "    \n",
        "    #add the exponentiated dot product to denom\n",
        "    weight_tran = np.transpose(weights)\n",
        "\n",
        "    denom += math.exp(np.matmul(weight_tran, x))\n",
        "  \n",
        "  #divide the exponentiated dot product of beta and x by denom\n",
        "  return math.exp(np.matmul(beta_tran, x))/denom\n",
        "\n",
        "\n",
        "#fits the model by running through each of the datapoints\n",
        "#and updating each weight vector using the step function\n",
        "\n",
        "#takes in x, the feature values, y, the labels and alpha,\n",
        "#the step size\n",
        "def fit_model(x, y, alpha):\n",
        "\n",
        "  #make an array for all of the weights, W\n",
        "  W = [[0] * np.shape(x)[1] for _ in range(7)]\n",
        "\n",
        "\n",
        "  #for each datapoint\n",
        "  for index in range(np.shape(x)[0]):\n",
        "\n",
        "    #update each weight vector in W\n",
        "    for idx in range(len(W)):\n",
        "\n",
        "      W[idx] = step(idx, x[index], y[index], alpha, W)\n",
        "  \n",
        "  return W\n",
        "\n",
        "#predict function takes in the feature values, x, the labels, y, \n",
        "#and W, the weight matrix produced by the model\n",
        "\n",
        "#returns the percent of classifications made by the model that were correct\n",
        "\n",
        "def predict(x, y, W):\n",
        "\n",
        "  numCorrect = 0\n",
        "\n",
        "  #for each datapoint\n",
        "  for index in range(len(x)):\n",
        "\n",
        "    #calculate the probability of the label being each of the classes\n",
        "    #given x\n",
        "    predictions = [hyp_func(beta, W, x[index]) for beta in W]\n",
        "\n",
        "    #get the index of the best prediction\n",
        "    classification = predictions.index(max(predictions))\n",
        "\n",
        "\n",
        "    \n",
        "    #if the index is equal to the decimal representation of the \n",
        "    #label for y, we correctly predicted the label\n",
        "    if classification == int(y[index], 2):\n",
        "      numCorrect += 1\n",
        "  \n",
        "  return 100*numCorrect/len(x)\n",
        "\n",
        "\n",
        "#convert the training set to numpy arrays\n",
        "xtrain_mat = xtrain.to_numpy()\n",
        "ytrain_mat = ytrain.to_numpy()\n",
        "\n",
        "#train the model on the set\n",
        "W = fit_model(xtrain_mat, ytrain_mat, 0.0001)\n",
        "\n",
        "#convert testing set to numpy array\n",
        "xtest_mat = xtest.to_numpy()\n",
        "ytest_mat = ytest.to_numpy()\n",
        "#find out what percent of classifications our model got right\n",
        "score = predict(xtest_mat, ytest_mat, W)\n",
        "\n",
        "\n",
        "\n",
        "print(\"For the dry beans dataset, our model was able to achieve an accuracy of\", score, \"% on the testing set.\")"
      ],
      "metadata": {
        "id": "y8DatYNdxNic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f09572-ffdc-467e-9b20-89b2b063d6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the dry beans dataset, our model was able to achieve an accuracy of 69.30983847283407 % on the testing set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JndMrbWlniMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Summary\n",
        "\n",
        "**Unregularized**\n",
        "\n",
        "On the South African heart disease dataset, our unregularlized model consistently had a percent correct in the range of 65% - 76%.\n",
        "\n",
        "On the dataset that we chose, a heart failure clinical record dataset (https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records), our unregularized model had a percent correct of around 80%.\n",
        "\n",
        "**Forward Stepwise**\n",
        "\n",
        "For our forward stepwise model, we aimed to select the features that were the most important to the model's performance. We did this by building all one feature models and selecting k models with the best score. After building all of the one feature models, we built all two feature models based off the best k models obtained in the previous step. After the last iteration, we select the best performing model, which dedcides on which k features are the best to include in our model. In our case, we decide on making k to equal 4 for easier comparison to the textbook's anaylsis done on the SA heart disease dataset. \n",
        "\n",
        "On the South African heart disease dataset, our model decided that the best features were: 'age', 'famhist', 'tobacco', and'sbp'. The percent correct fell into a similar range as our unregularized model, but noticably never fell below 60% across different train-test shuffles. \n",
        "\n",
        "On the dataset we chose, our model decided that the best features were: 'time', 'high_blood_pressure', 'serum_sodium', 'anaemia'. The forward stepwise performed worse than our other models, having a percent correct of around 70%.\n",
        "\n",
        "**L1 Regularized**\n",
        "\n",
        "For our L1 regularized model, we used the update method the paper (https://aclanthology.org/P09-1054.pdf) referred to as \"SDG L1 (Clipping)\". This method involves first updating the weight without the L1 penalty term. Then, we add the derivative (+- lambda) of the L1 penalty. However, if there is a sign change, the weight is \"clipped\" to 0.\n",
        "\n",
        "For both the South African heart disease dataset and the dataset that we chose, the percent correct tended to fall within the same percent range as our unregularized model: 65%-76% for the SA heart dataset, and ~80% for the heart failure dataset. \n",
        "\n",
        "Using the validation dataset and the Lasso plot we produced, we got optimal lambdas in a range from 0 to 0.3, and the most important features were sbp, age, adiposity, and famhist. These results are fairly similar to the results we obtained from our forward stepwise model, where features age, fhamist, tobacco and sbp were selected. The two models' selected features were only off by one, differing by tobacco and adiposity.\n",
        "\n",
        "The textbook's analysis done on the South African heart disease dataset determined that the optimal features to include in the model were tobacco, ldl, famhist, and age. Comparing these results to the optimal features selected from our L1 regularized model and our step forward model, our L1 regularized model's selected features were off by two (sbp and adiposity vs tobacco and ldl) and our step forward model's selecte features were off by 1 (ldl and sbp).\n",
        "\n",
        "**L2 Regularized**\n",
        "\n",
        "For the L2 regularized model, we added the gradient of the L2 penalty to the update function. We found that the optimal lambdas for the L2 regularized models ranged from 0 to 0.16. \n",
        "\n",
        "For both the South African heart disease dataset and the dataset that we chose, the percent correct tended to fall within the same percent range as our unregularized and L1 regularized model: 65%-76% for the SA heart dataset, and ~80% for the heart failure dataset. \n",
        "\n",
        "**Unregularized Multinomal Regression**\n",
        "\n",
        "We extended our unregularized model to mutlinomial regression and tested it on a dry bean dataset from the UCI repository where features such as shape and perimeter are used to predict the type of dry bean.\n",
        "The primary difference between binomial and multinomial regression is that each class (the type of dry bean in this case) now gets their own weight vector, and both the update function and hypothesis functions change. The denominator of the hypothesis function in the multinominal case becomes the sum of e raised to the power of the dot product of each of the weight vectors with x. The new update formula updates weight vectors based on the index that is provided to the function. The new update formula and the hypothesis function used for our implmentation of multinominal regression was taken from section 4.3.4 of a ML text by Bishop (http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)\n",
        "\n",
        "There are seven types of dry beans that the model is aiming to predict. Therefore, we encoded our labels into binary strings, where each dry bean would be represented in combinations of three bits. We later converted the binary strings to decimal when necessary in the update function to check which class a label belonged to. \n",
        "\n",
        "Our unregularized, multinominal model produced a percent correct of around 70% percent across different train-test shuffles. "
      ],
      "metadata": {
        "id": "AwAaTv6AfKUI"
      }
    }
  ]
}